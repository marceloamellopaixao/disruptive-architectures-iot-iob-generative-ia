{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b0PwlVawUlYk"
      },
      "source": [
        "## APRENDIZADO DE MÁQUINA NÃO SUPERVISIONADO:\n",
        "\n",
        "## Redução de dimensionalidade utilizando o algoritmo PCA (Principal Componentes Analysis)\n",
        "\n",
        "O PCA é um dos principais algoritmos de aprendizagem de máquina não supervisionada.\n",
        "\n",
        "Ele identifica a correlação entre variáveis e, caso haja forte correlação entre elas, decide que é possível reduzir a dimensionalidade dos dados.\n",
        "\n",
        "Supondo um número m de variáveis, o PCA extrai um novo número p <= m que explica a melhor variação na base de dados, excluindo a variável dependente y. Tal número m é escolhido pelo projetista.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Parte 1: Importação de bibliotecas"
      ],
      "metadata": {
        "id": "5y3z-B4i6WkO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Conversão de dados categóricos em numéricos:\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "# https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html\n",
        "# Sugestão de leitura: Como fazer LabelEncoder em um DataFrame — Python/SKLearn\n",
        "# https://gianmedeirao.medium.com/como-fazer-labelencoder-em-um-dataframe-python-sklearn-655ba2c6ae7e\n",
        "\n",
        "# Normalização dos dados\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "# https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html\n",
        "# Sugestão de leitura: How to Use StandardScaler and MinMaxScaler Transforms in Python:\n",
        "# https://machinelearningmastery.com/standardscaler-and-minmaxscaler-transforms-in-python/\n",
        "# https://www.hashtagtreinamentos.com/padronizacao-e-normalizacao-em-ciencia-de-dados\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "# https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html\n",
        "# Sugestão de leitura:\n",
        "# https://www.geeksforgeeks.org/how-to-do-train-test-split-using-sklearn-in-python/\n",
        "\n",
        "from sklearn.decomposition import PCA\n",
        "# https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html\n",
        "\n",
        "# Classificador Random Forest\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "# https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html\n",
        "\n",
        "# Pacote para cálculo da acurácia do modelo de classificação:\n",
        "from sklearn.metrics import accuracy_score\n",
        "# https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html"
      ],
      "metadata": {
        "id": "eXftjh2BfSm_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5B4KH9_YVrmQ"
      },
      "source": [
        "# Parte 2: Preparação dos dados\n",
        "\n",
        "O conjunto de dados modificado \"census.csv\" consiste em aproximadamente 32.000 registros de dados, com cada registro de dados tendo 13 características.\n",
        "\n",
        "https://archive.ics.uci.edu/dataset/20/census+income\n",
        "\n",
        "Este conjunto de dados é uma versão modificada do conjunto de dados publicado no artigo \"*Scaling Up the Accuracy of Naive-Bayes Classifiers: a Decision-Tree Hybrid*\", de Ron Kohavi.\n",
        "\n",
        "Artigo:\n",
        "\n",
        "https://aaai.org/papers/033-scaling-up-the-accuracy-of-naive-bayes-classifiers-a-decision-tree-hybrid/\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DsluTWi5V_FP"
      },
      "outputs": [],
      "source": [
        "# Criando um dataframe a partir do dados do arquivo\n",
        "df = pd.read_csv('/content/census.csv')\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Seleção de atributos (Dados de entrada)\n",
        "# Com o método .iloc selecionamos (por índice) todas as linhas,\n",
        "# e as colunas de 1 a 13 (o segundo parâmetro é excludente)\n",
        "X = df.iloc[:, 0:14].values"
      ],
      "metadata": {
        "id": "BTi4Ml0ffrNV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X"
      ],
      "metadata": {
        "id": "r65JXWoI_S5K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Seleção do target (Dados de saída)\n",
        "# Com o método .iloc selecionamos (por índice) todas as linhas, apenas a coluna 14 (income)\n",
        "y = df.iloc[:, 14].values"
      ],
      "metadata": {
        "id": "LhZwmixLfvQu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Em Machine Learning, o \"Label Encoding\" é o processo de conversão\n",
        "# de variáveis categóricas em variáveis numéricas (inteiros).\n",
        "\n",
        "# Em nosso dataframe, as colunas com variáveis categóricas (e seus índices) são:\n",
        "# workclass (1), education (3), marital (5), occupation (6), relationship (7), race (8), sex (9) e country (13)\n",
        "\n",
        "# Criaremos variáveis com o nome das colunas e aplicaremos a transformação para cada uma deles com o LabelEncoder():\n",
        "le_workclass = LabelEncoder()\n",
        "le_education = LabelEncoder()\n",
        "le_marital = LabelEncoder()\n",
        "le_occupation = LabelEncoder()\n",
        "le_relationship = LabelEncoder()\n",
        "le_race = LabelEncoder()\n",
        "le_sex = LabelEncoder()\n",
        "le_country = LabelEncoder()"
      ],
      "metadata": {
        "id": "rjG5hM_Ef0r3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Usando o método .fit_transform aplicamos os novos valores transformados às colunas:\n",
        "X[:,1] = le_workclass.fit_transform(X[:,1])\n",
        "X[:,3] = le_education.fit_transform(X[:,3])\n",
        "X[:,5] = le_marital.fit_transform(X[:,5])\n",
        "X[:,6] = le_occupation.fit_transform(X[:,6])\n",
        "X[:,7] = le_relationship.fit_transform(X[:,7])\n",
        "X[:,8] = le_race.fit_transform(X[:,8])\n",
        "X[:,9] = le_sex.fit_transform(X[:,9])\n",
        "X[:,13] = le_country.fit_transform(X[:,13])"
      ],
      "metadata": {
        "id": "Ag9BYqh4f7T5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X"
      ],
      "metadata": {
        "id": "fmptGzvIf_Bj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Padronização\n",
        "# É o redimensionamento dos recursos para criar um “padrão” garantindo\n",
        "# que os novos dados tenham média zero e desvio padrão igual a 1\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Salvando os novos dados já padronizados:\n",
        "X = scaler.fit_transform(X)"
      ],
      "metadata": {
        "id": "jdc5VIv8gCW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X"
      ],
      "metadata": {
        "id": "1kL1W76dgH1m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Com o método train_test_split separamos os dados em treino e teste:\n",
        "# X_treino e y_treino: dados para treinamento dos modelos (80% dos dados iniciais)\n",
        "# X_teste e y_teste: dados para teste e avaliação dos modelos (20% dos dados iniciais)\n",
        "\n",
        "X_treino, X_teste, y_treino, y_teste = train_test_split(X, y, test_size=0.20)"
      ],
      "metadata": {
        "id": "GRYXDQPngKM_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_treino.shape, X_teste.shape"
      ],
      "metadata": {
        "id": "zujBwX-1gPnf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6OurC4GyUoak"
      },
      "source": [
        "## Parte 3: Aplicação do algoritmo PCA (Principal component analysis)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZmpO8urpX8kz"
      },
      "outputs": [],
      "source": [
        "# Criamos a variável pca e aplicamos o algoritmo PCA para a redução do número original de colunas para\n",
        "# um número definido \"p\"\n",
        "# O hiperparâmetro \"n_components\" recebe o valor definido de novas colunas (p)\n",
        "p = 8\n",
        "pca = PCA(n_components=p)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zVHnZ8rYYOeL"
      },
      "outputs": [],
      "source": [
        "# Definimos novas variáveis de treino e testes (agora com um menor números de colunas)\n",
        "X_treino_pca = pca.fit_transform(X_treino)\n",
        "X_teste_pca = pca.transform(X_teste)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Ouz2wdFYkJh"
      },
      "outputs": [],
      "source": [
        "X_treino_pca.shape, X_teste_pca.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hdBAi-piY28x"
      },
      "outputs": [],
      "source": [
        "# O método \"explained_variance_ratio\" permite calcular a\n",
        "# porcentagem de variância dos dados que consegue ser explicada por cada um dos componentes selecionados\n",
        "\n",
        "# Relembrando! :-)\n",
        "# A variância, assim como o desvio padrão, é uma das medidas de dispersão de dados\n",
        "# que mostra o comportamento dos dados de uma amostra em relação a uma medida central (por exemplo, a média)\n",
        "\n",
        "pca.explained_variance_ratio_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lBxPFP0VZR1I"
      },
      "outputs": [],
      "source": [
        "# Somando os percentuais conseguimos ver o quanto nosso\n",
        "# modelo PCA conseguiu explicar dos dados de entrada\n",
        "total_variancia = pca.explained_variance_ratio_.sum()\n",
        "print(f'O modelo PCA com {p} variáveis explica{100*(total_variancia): .2f} % dos dados de entrada')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Parte 4: Aplicando os novos dados gerados em um modelo de classificação."
      ],
      "metadata": {
        "id": "GHyjEUMJOTJv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Utilizaremos o algoritmo de classificação Random Forest com base nos novos dados, após a aplicação da técnica de redução de dimensionalidade."
      ],
      "metadata": {
        "id": "HTuaKv4_x6eN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "icbXkzMbaNu0"
      },
      "outputs": [],
      "source": [
        "# Instanciando o objeto do classificador\n",
        "modelo_rf = RandomForestClassifier(n_estimators=40, random_state=0)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Usando o método .fit para treinar o modelo\n",
        "\n",
        "# Com base nos dados de treino iniciais:\n",
        "modelo_rf.fit(X_treino, y_treino)\n",
        "\n",
        "# Com base nos dados de treino gerados após a aplicação do algoritmo PCA:\n",
        "# modelo_rf.fit(X_treino_pca, y_treino)"
      ],
      "metadata": {
        "id": "Z4rFYXXmYJbG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B4b0SxRvacc1"
      },
      "outputs": [],
      "source": [
        "# Usando o método .predict para estimar os resultados\n",
        "\n",
        "# Realizando as previsões com base nos dados de teste iniciais\n",
        "previsoes = modelo_rf.predict(X_teste)\n",
        "\n",
        "# Realizando as previsões com base nos dados de teste após a aplicação do algoritmo PCA\n",
        "# previsoes = modelo_rf.predict(X_teste_pca)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "previsoes"
      ],
      "metadata": {
        "id": "ugsXlY5QbmRD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L8TTOcS0ai60"
      },
      "outputs": [],
      "source": [
        "y_teste"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p585sPd2aoXI"
      },
      "outputs": [],
      "source": [
        "# Calculando a acurácia do modelo de classificação:\n",
        "acuracia = accuracy_score(y_teste, previsoes)\n",
        "print(f'A acurácia do modelo Random Forest é de:{(100*acuracia): .2f} %')\n",
        "\n",
        "# Sugestão de leitura:\n",
        "# Métricas de avaliação de modelos de aprendizado de máquina:\n",
        "# https://mariofilho.com/as-metricas-mais-populares-para-avaliar-modelos-de-machine-learning/"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Parte 5: Sugestões de exercícios\n",
        "\n",
        "- Volte nas células da Parte 3 e altere os valores de p.\n",
        "\n",
        "- Recalcule a média de variância dos dados após a aplicação do algoritmo PCA.\n",
        "\n",
        "- Treine novamente o modelo de classificação, realize as predições e compare os resultados.\n",
        "\n",
        "- Utilize um novo dataset e repita os passos."
      ],
      "metadata": {
        "id": "ZI36tPxIcVHb"
      }
    }
  ]
}